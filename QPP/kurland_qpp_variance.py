import sys
import re
import xml.etree.ElementTree as ET
from gensim.parsing import remove_stopwords
from nltk.stem import PorterStemmer
from math import sqrt
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import pearsonr, kendalltau

if len(sys.argv) < 11:
    print('Needs 10 arguments - \n1. TREC initial query based LM ranked file (reranked by DRMM)\n'
          '2. UQV-TREC query variants based LM ranked file (reranked by DRMM)\n'
          '3. TREC initial query .AP file\n'
          '4. TREC query file\n'
          '5. Query variant file (UQV queries)\n'
          '6. res file path\n'
          '7. No. of top documents you want to take from the initial.res file (retrieved by initial query)\n'
          '8. No. of top documents retrieved by query variant in the variant.res file (generated by LM and '
          're-ranked by DRMM)\n'
          '9. No. of top scores you want to consider for each variant\n'
          '10. Value of qmix [0.1-0.9]')
    exit(0)

arg_trec_prerank_file = sys.argv[1]
arg_uqv_prerank_file = sys.argv[2]
arg_trec_ap_file = sys.argv[3]
arg_trec_query_file = sys.argv[4]
arg_query_variant_file = sys.argv[5]
arg_res_path = sys.argv[6]
arg_top_docs_iq = int(sys.argv[7])
arg_top_docs_vq = int(sys.argv[8])
arg_top_scores_vq = int(sys.argv[9])
arg_qmix = float(sys.argv[10])

# fw_variance = open(arg_res_path + 'TD-' + str(arg_top_docs_iq) + 'TS-' + str(arg_top_docs_vq) +
#                    '-qmix-' + str(arg_qmix) + '_variance.res', 'w')
# fw_gini = open(arg_res_path + 'TD-' + str(arg_top_docs_iq) + 'TS-' + str(arg_top_docs_vq) +
#                '-qmix-' + str(arg_qmix) + '_gini.res', 'w')

query_similarity = {}
trec_v_Q = {}
trec_g_Q = {}
uqv_v_Q = {}
uqv_g_Q = {}
nqc_v_dict = {}
nqc_g_dict = {}


def compute_variance_trec(score_list, qid):
    variance = 0
    score_list = np.array(score_list).astype(np.float)
    mu = sum(score_list) / len(score_list)
    # print("mu : ", mu)
    for score in score_list:
        variance += pow((score - mu), 2)
        # print("variance : ", variance)
    variance = sqrt(1/len(score_list) * variance)
    # print("variance here : ", variance)
    trec_v_Q[qid] = round(variance, 6)
    # print("TREC res file variance dict : ", trec_v_Q)


def variance_trec_query(trec_res_file, topdocs):
    qid = ""
    count = 0
    per_query_score_list = []
    fp = open(trec_res_file)
    for line in fp.readlines():
        parts = line.split('\t')
        if qid == "" or parts[0] == qid:
            if count < topdocs:
                qid = parts[0]
                score = parts[4]
                per_query_score_list.append(score)
                count = count + 1
        elif parts[0] != qid:
            # print('query : ', qid, '\t', per_query_score_list)
            compute_variance_trec(per_query_score_list, qid)
            per_query_score_list = []
            count = 0
            qid = parts[0]
            score = parts[4]
            per_query_score_list.append(score)
            count = count + 1
    # print('query : ', qid, '\t', per_query_score_list)
    compute_variance_trec(per_query_score_list, qid)


def compute_variance_uqv(score_list, varscores):
    # print("sorted list size : ", len(score_list))
    score_list = np.array(score_list).astype(np.float)
    score_list = score_list[0:varscores]
    # print('sub-score list size : ', len(score_list))
    mu = sum(score_list) / len(score_list)
    # print("mu one uqc : ", mu)
    variance = 0
    for score in score_list:
        variance += pow((score - mu), 2)
        # print("var : ", variance)
    variance = sqrt(variance * (1 / len(score_list)))
    # print("variance one uqc : ", variance)
    return round(variance, 6)


def variance_uqv_query(uqv_res_file, topdocs, varscores):
    qid = ""
    count = 0
    fp = open(uqv_res_file)
    per_query_score_list = []
    per_query_QV_list = []
    for line in fp.readlines():
        parts = line.split('\t')
        if qid == "" or parts[0] == qid:
            if count < topdocs:
                per_query_score_list.append(parts[4])
                qid = parts[0]
                count += 1
            else:
                # print('query : ', qid, '\t', 'score list size : ', len(per_query_score_list))
                per_query_score_list = sorted(per_query_score_list, reverse=True)
                variance = compute_variance_uqv(per_query_score_list, varscores)
                per_query_QV_list.append(variance)
                count = 1
                qid = parts[0]
                per_query_score_list = [parts[4]]
        elif parts[0] != qid:
            # print('query : ', qid, '\t', 'score list size : ', len(per_query_score_list))
            per_query_score_list = sorted(per_query_score_list, reverse=True)
            variance = compute_variance_uqv(per_query_score_list, varscores)
            per_query_QV_list.append(variance)
            uqv_v_Q[qid] = per_query_QV_list
            # print('query : ', qid, '\t', 'no. of query variants : ', len(per_query_QV_list))
            count = 1
            qid = parts[0]
            per_query_score_list = [parts[4]]
            per_query_QV_list = []
    # print('query : ', qid, '\t', 'score list size : ', len(per_query_score_list))
    per_query_score_list = sorted(per_query_score_list, reverse=True)
    variance = compute_variance_uqv(per_query_score_list, varscores)
    per_query_QV_list.append(variance)
    uqv_v_Q[qid] = per_query_QV_list
    # print('query : ', qid, '\t', 'no. of query variants : ', len(per_query_QV_list))
    # print("final list : ", uqv_v_Q)


def compute_gini_trec(score_list, qid):
    i = 1
    weighted_sum = 0
    gini_array = np.array(score_list).astype(np.float)
    gini_array.sort()
    # print(gini_array)
    gini_array_size = len(gini_array)
    # print(gini_array_size)
    coef = 2 / gini_array_size
    # print(coef)
    const = (gini_array_size + 1) / gini_array_size
    # print(const)
    while i <= gini_array_size:
        for yi in gini_array:
            weighted_sum += i * yi
            i += 1
    # print(weighted_sum, "\t", i)
    gini = ((coef * weighted_sum) / gini_array.sum()) - const
    # print("gini : ", gini)
    trec_g_Q[qid] = round(gini, 4)


def gini_trec_query(uqv_res_file, topdocs):
    qid = ""
    count = 0
    per_query_score_list = []
    fp = open(uqv_res_file)
    for line in fp.readlines():
        parts = line.split('\t')
        if qid == "" or parts[0] == qid:
            if count < topdocs:
                qid = parts[0]
                score = parts[3]
                per_query_score_list.append(score)
                count = count + 1
        elif parts[0] != qid:
            # print('query : ', qid, '\t', per_query_score_list)
            compute_gini_trec(per_query_score_list, qid)
            per_query_score_list = []
            count = 0
            qid = parts[0]
            score = parts[3]
            per_query_score_list.append(score)
            count = count + 1
    # print('query : ', qid, '\t', per_query_score_list)
    compute_gini_trec(per_query_score_list, qid)


def compute_gini_uqv(score_list, qid, topdocs):
    uqv_gini_list = []
    itr = 0
    score_list = np.array(score_list).astype(np.float)
    no_query_variant = int(len(score_list) / topdocs)
    # print("qID : ", qid, "\tno of query variants : ", no_query_variant)
    while no_query_variant > 0:
        scores = score_list[topdocs * itr:topdocs * itr + topdocs]
        # print("score list one uqc : ", scores)
        scores_size = len(scores)
        scores.sort()
        # print("sorted list :", scores)
        i = 1
        weighted_sum = 0
        coef = 2 / scores_size
        const = (scores_size + 1) / scores_size
        while i < scores_size + 1:
            for yi in scores:
                weighted_sum += i * yi
                # print("here : ", weighted_sum)
                i += 1
        # print("weighted sum : ", weighted_sum)
        gini = ((coef * weighted_sum) / scores.sum()) - const
        # print("gini : ", gini)
        uqv_gini_list.append(round(gini, 4))
        itr += 1
        no_query_variant -= 1
    uqv_g_Q[qid] = uqv_gini_list
    # print("dict : ", uqv_g_Q)


def gini_uqv_query(uqv_res_file, topdocs):
    qid = ""
    # count = 0
    per_query_score_list = []
    fp = open(uqv_res_file)
    for line in fp.readlines():
        parts = line.split('\t')
        if qid == "" or parts[0] == qid:
            qid = parts[0]
            score = parts[4]
            per_query_score_list.append(score)
            # count = count + 1
        elif parts[0] != qid:
            # print('query : ', qid, '\t', per_query_score_list)
            compute_gini_uqv(per_query_score_list, qid, topdocs)
            per_query_score_list = []
            # count = 0
            qid = parts[0]
            score = parts[4]
            per_query_score_list.append(score)
            # count = count + 1
    # print('query : ', qid, '\t', per_query_score_list)
    compute_gini_uqv(per_query_score_list, qid, topdocs)


# =============== compute inter-query association p'(q'|q) ==============

def jaccard_similarity(initial_dict, var_dict):
    stemmer = PorterStemmer()
    for qid in initial_dict:
        similarity_list = []
        initial = remove_stopwords(initial_dict[qid])
        initial_stem = stemmer.stem(initial.lower().strip())
        initial_set = set(initial_stem.split())
        # print("initial set : ", initial_set)
        variant_list = var_dict[qid]
        # print("var list : ", variant_list)
        for var in variant_list:
            # print("one var : ", var)
            variant = remove_stopwords(var)
            variant_stem = stemmer.stem(variant.lower().strip())
            variant_set = set(variant_stem.split())
            # print("var set : ", variant_set)
            intersec = initial_set.intersection(variant_set)
            # print("intersection : ", intersec)
            similarity = round(float(len(intersec)) / (len(initial_set) + len(variant_set) - len(intersec)), 4)
            # print("similarity : ", similarity)
            similarity_list.append(similarity)
        query_similarity[qid] = np.array(similarity_list).astype(float)
    # print("inter query similarity : ", query_similarity)


def inter_query_association_uqv(initial_query, query_variants):
    initial_query_dict = {}
    variant_dict = {}
    rootElement = ET.parse(initial_query).getroot()
    for subElement in rootElement:
        query = re.sub('[^a-zA-Z0-9\n\.]', ' ', subElement[1].text)
        initial_query_dict[subElement[0].text.strip()] = query
    # print(initial_query_dict)

    fp = open(query_variants)
    qid = ''
    per_query_var_list = []
    for line in fp.readlines():
        parts = line.split('-')
        if qid == '' or parts[0] == qid:
            qid = parts[0]
            variant = parts[2].split(';')
            variant = variant[1].replace('\n', '')
            variant_clean = re.sub('[^a-zA-Z0-9\n\.]', ' ', variant)
            # print(variant_clean)
            per_query_var_list.append(variant_clean)
        elif parts[0] != qid:
            variant_dict[qid] = per_query_var_list
            qid = parts[0]
            variant = parts[2].split(';')
            variant = variant[1].replace('\n', '')
            variant_clean = re.sub('[^a-zA-Z0-9\n\.]', ' ', variant)
            # print(variant_clean)
            per_query_var_list = [variant_clean]
    variant_dict[qid] = per_query_var_list
    # print(variant_dict)
    jaccard_similarity(initial_query_dict, variant_dict)


def inter_query_association_dsd(initial_query, query_variants):
    initial_query_dict = {}
    variant_dict = {}
    rootElement = ET.parse(initial_query).getroot()
    for subElement in rootElement:
        query = re.sub('[^a-zA-Z0-9\n\.]', ' ', subElement[1].text)
        initial_query_dict[subElement[0].text.strip()] = query
    # print(initial_query_dict)

    fp = open(query_variants)
    qid = ''
    per_query_var_list = []
    for line in fp.readlines():
        parts = line.split('\t')
        if qid == '' or parts[0] == qid:
            qid = parts[0]
            variant = parts[1].replace('\n', '')
            variant_clean = re.sub('[^a-zA-Z0-9\n\.]', ' ', variant)
            # print(variant_clean)
            per_query_var_list.append(variant_clean)
        elif parts[0] != qid:
            variant_dict[qid] = per_query_var_list
            qid = parts[0]
            variant = parts[1].replace('\n', '')
            variant_clean = re.sub('[^a-zA-Z0-9\n\.]', ' ', variant)
            # print(variant_clean)
            per_query_var_list = [variant_clean]
    variant_dict[qid] = per_query_var_list
    # print(variant_dict)
    jaccard_similarity(initial_query_dict, variant_dict)


# inter_query_association_uqv(arg_trec_query_file, arg_query_variant_file)  # for UQV (manual query variants)
inter_query_association_dsd(arg_trec_query_file, arg_query_variant_file)  # for DSD (our variants obtained using RM and W2V)

# ================ calculate NQC_v : del_avg(Q,Q') = del_v(Q') - del_v(Q) / del_v(Q) ==================

variance_trec_query(arg_trec_prerank_file, arg_top_docs_iq)
# print("\nTREC res file variance dict : ", trec_v_Q)
variance_uqv_query(arg_uqv_prerank_file, arg_top_docs_vq, arg_top_scores_vq)
# print("\nUQV res file variance dict : ", uqv_v_Q)

only_ref = 0
for key in trec_v_Q:
    # print("\nkey : ", key)
    q_dash = uqv_v_Q[key]
    # print("qdash : ", q_dash)
    similarity = query_similarity[key]
    # print("similarity list : ", similarity)
    res_list = []
    for i in range(0, len(q_dash)):
        res_list.append(q_dash[i] * similarity[i])
    only_ref = sum(res_list) / len(q_dash)
    # print("only ref : ", only_ref)
    only_ref = arg_qmix * trec_v_Q[key] + (1 - arg_qmix) * only_ref
    # print("now only ref : ", only_ref)
    nqc_v_dict[key] = round(only_ref, 4)
print("\nP_onlyref(Q) : ", nqc_v_dict)
#
# # =============== compute rho & tau for NQC_v ====================

fp = open(arg_trec_ap_file)
ap_scores = []
nqc_v_scores = []
for line in fp.readlines():
    ap_scores.append(float(line.split('\t')[1]))
for key in nqc_v_dict:
    nqc_v_scores.append(nqc_v_dict[key])

xranks = pd.Series(ap_scores).rank()
# print("Rankings of X:", xranks)
yranks = pd.Series(nqc_v_scores).rank()
# print("Rankings of Y:", yranks)
rho, _ = stats.spearmanr(ap_scores, nqc_v_scores)
# print("\nSpearman's Rank correlation:", round(rho, 4))

tau, _ = stats.kendalltau(ap_scores, nqc_v_scores)
# print('Kendall Rank correlation: %.5f' % tau)
print(round(rho, 4), '\t', round(tau, 4))

# for qid, nqc_v in nqc_v_dict.items():
#     fw_variance.writelines(str(qid) + '\t' + str(nqc_v) + '\n')
# fw_variance.writelines('\n\n' + str(rho) + '\t' + str(tau))
# fw_variance.close()

# # ================ calculate NQC_g : del_avg(Q,Q') = del_v(Q') - del_v(Q) / del_v(Q) ==================
#
# gini_trec_query(arg_trec_prerank_file, arg_top_docs)
# print("\nTREC res file gini dict : ", trec_g_Q)
# gini_uqv_query(arg_uqv_prerank_file, arg_top_docs)
# print("\nUQV res file gini dict : ", uqv_g_Q)
#
# only_ref = 0
# for key in trec_g_Q:
#     # print("\nkey : ", key)
#     q_dash = uqv_g_Q[key]
#     # print("qdash : ", q_dash)
#     only_ref = sum(q_dash) / len(q_dash)
#     # print("only ref : ", only_ref)
#     only_ref = arg_qmix * trec_g_Q[key] + (1 - arg_qmix) * only_ref
#     # print("now only ref : ", only_ref)
#     nqc_g_dict[key] = round(only_ref, 4)
# print("\nP_onlyref(Q) : ", nqc_g_dict)
#
# # =============== compute rho & tau for NQC_v ====================
#
# nqc_g_scores = []
# for key in nqc_v_dict:
#     nqc_g_scores.append(nqc_g_dict[key])
#
# xranks = pd.Series(ap_scores).rank()
# # print("Rankings of X:", xranks)
#
# yranks = pd.Series(nqc_g_scores).rank()
# # print("Rankings of Y:", yranks)
# rho = scipy.stats.pearsonr(xranks, yranks)[0]
# print("\nSpearman's Rank correlation:", rho)
#
# tau, _ = kendalltau(ap_scores, nqc_g_scores)
# print('Kendall Rank correlation: %.5f' % tau)
#
# for qid, nqc_g in nqc_g_dict.items():
#     fw_gini.writelines(str(qid) + '\t' + str(nqc_g) + '\n')
# fw_gini.writelines('\n\n' + str(rho) + '\t' + str(tau))
# fw_gini.close()
